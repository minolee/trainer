
model_name: test_llama31_sft
reader:
  sources:
    - name: test
      source: rsc/data/AiHub-Largeai_SFT_QA.jsonl
      split:
        type: mixed
        split_ratio: "9:1"
      limit: 1000
      reader: read_simple
    # - name: test2
    #   source: rsc/data/single_lines.jsonl
    #   split:
    #     type: train
    #   limit: 100
    #   reader: read_simple

dataset:
  prompt: Llama31
  dataset: 
    name: SFTDataset
    max_length: 4096

dataloader:
  shuffle: true
  num_workers: 0

tokenizer:
  path: /home/work/user/alignment/model/v2.0/Midm_v2.0_Inst/merged_checkpoint

model:
  path: /home/work/user/alignment/model/v2.0/Midm_v2.0_Inst/merged_checkpoint
  device: cuda


loss:
  name: CrossEntropyLoss
  ignore_index: -100

optimizer:
  name: adamw_hf
  learning_rate: 5.0e-5
  weight_decay: 0.01

scheduler:
  name: linear
  warmup_steps: 100

training_arguments:
  num_train_epochs: 1
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 4
  log_level: info
  save_only_model: true
  gradient_checkpointing: true # set true to save more memory
  # report_to: tensorboard
  deepspeed:
    num_gpus: 8
    zero_optimization:
      stage: 3
      offload_param:
        device: cpu
        pin_memory: true
      offload_optimizer:
        device: cpu
        pin_memory: true
    # 여기서 batch_size를 정의하지 말고 training_arguments에서 정의할 것
    gradient_clipping: 1.0
    fp16: 
      enabled: true