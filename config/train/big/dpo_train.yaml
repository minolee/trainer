
model_name: test_llama31_sft
base_trainer: DPOTrainer
reader:
  sources:
    - name: test
      source: rsc/data/preference/helpfulness/dpo_1cycle_241016.jsonl
      split:
        type: train
      limit: 1000
    - name: test2
      source: rsc/data/preference/helpfulness/dpo_kt-IF_v1_241213.jsonl
      split:
        type: train
      limit: 1000
  reader: read_preference

dataset:
  prompt: Llama31
  dataset: 
    name: PreferenceDataset
    max_length: 4096

dataloader:
  shuffle: true
  num_workers: 0
  collate_fn: preference_collate_fn

tokenizer:
  path: /home/work/user/alignment/model/v2.0/Midm_v2.0_Inst/merged_checkpoint

model:
  path: /home/work/user/alignment/model/v2.0/Midm_v2.0_Inst/merged_checkpoint
  device: cuda

ref_model:
  path: /home/work/user/alignment/model/v2.0/Midm_v2.0_Inst/merged_checkpoint
  device: cuda

optimizer:
  name: adamw_hf
  learning_rate: 5.0e-5
  weight_decay: 0.01

scheduler:
  name: linear
  warmup_steps: 100

training_arguments:
  num_train_epochs: 1
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 4
  log_level: info
  save_only_model: true
  gradient_checkpointing: true # set true to save more memory
  # report_to: tensorboard
  # deepspeed:
  #   num_gpus: 8
  #   zero_optimization:
  #     stage: 3
  #     offload_param:
  #       device: cpu
  #       pin_memory: true
  #     offload_optimizer:
  #       device: cpu
  #       pin_memory: true
  #   # 여기서 batch_size를 정의하지 말고 training_arguments에서 정의할 것
  #   gradient_clipping: 1.0
  #   fp16: 
  #     enabled: auto