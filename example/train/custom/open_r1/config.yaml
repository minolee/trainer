

model_name: midm_r1_zero
trainer: 
  name: GRPOTrainer
  bf16: true
  use_vllm: false
  do_eval: false
  eval_strategy: steps
  eval_steps: 100
  gradient_accumulation_steps: 16
  gradient_checkpointing: true
  gradient_checkpointing_kwargs:
    use_reentrant: false
  learning_rate: 2.0e-05
  log_level: info
  logging_steps: 5
  logging_strategy: steps
  
  max_prompt_length: 512
  max_completion_length: 1024
  max_steps: -1
  num_generations: 2
  num_train_epochs: 1
  overwrite_output_dir: true
  per_device_eval_batch_size: 4   
  per_device_train_batch_size: 2
  report_to:
  - wandb
  save_strategy: "no"
  seed: 42
  warmup_ratio: 0.1
  reward_funcs:
    - reward.accuracy_reward
    - reward.format_reward

data:
  sources:
    - source: AI-MO/NuminaMath-TIR
      use_cache: true
      reader: reader.read_sol
      limit: 500
    - source: AI-MO/NuminaMath-TIR
      use_cache: true
      use_as: dev
      reader: reader.read_sol
      split: train[-100:]

  formatter: reader.format_conversation

model: 
  path: Qwen/Qwen2-0.5B-Instruct
  torch_dtype: bfloat16
  device: mps
